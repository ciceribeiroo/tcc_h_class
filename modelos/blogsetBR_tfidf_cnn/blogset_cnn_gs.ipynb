{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-11 16:00:02.825368: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-11 16:00:02.825433: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import tensorflow \n",
    "tensorflow.random.set_seed(1) \n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikeras in /home/alice/.local/lib/python3.9/site-packages (0.8.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /home/alice/.local/lib/python3.9/site-packages (from scikeras) (1.1.1)\n",
      "Requirement already satisfied: packaging<22.0,>=0.21 in /home/alice/.local/lib/python3.9/site-packages (from scikeras) (21.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/alice/.local/lib/python3.9/site-packages (from scikit-learn>=1.0.0->scikeras) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/alice/.local/lib/python3.9/site-packages (from scikit-learn>=1.0.0->scikeras) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/alice/.local/lib/python3.9/site-packages (from scikit-learn>=1.0.0->scikeras) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/alice/.local/lib/python3.9/site-packages (from scikit-learn>=1.0.0->scikeras) (1.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/alice/.local/lib/python3.9/site-packages (from packaging<22.0,>=0.21->scikeras) (3.0.9)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement libcudart (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for libcudart\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install libcudart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[60]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[60]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alice/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D, Convolution1D\n",
    "from keras.optimizers import Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ageGroups(text):\n",
    "    if text == '10 à 18 anos' or text=='19 à 25 anos':\n",
    "        return 0\n",
    "    elif text=='26 à 30 anos' or text=='30 à 40 anos':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    all_words = text.split(\" \")\n",
    "    clean_text = [i for i in all_words if i not in stopwords and i!=\"\"]\n",
    "    return \" \".join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_texts(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    clean_text = remove_stopwords(text)\n",
    "    clean_text = remove_html_tags(clean_text)\n",
    "    clean_text = lower_texts(clean_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genderClassifier(text):\n",
    "    if text == 'Masculino':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(filters = [100], kernel_size = [50], strides = [100], \n",
    "                 dropout_rate = 0.5, pool_size = [5], dense_units = 100, max_len = 1000):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # conv 1\n",
    "    model.add(Conv1D(filters = filters[0], \n",
    "                     kernel_size = kernel_size[0],\n",
    "                     strides = strides[0], \n",
    "                     activation = 'relu', \n",
    "                     input_shape = (max_len, 1) ))\n",
    "\n",
    "    # pooling layer 1\n",
    "    for i in range(len(pool_size)):\n",
    "        model.add(MaxPooling1D(pool_size = pool_size[i], strides = 1))\n",
    "        model.add(Activation('relu'))\n",
    "    \n",
    "    #model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    if dropout_rate is not None:\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "    model.add(Dense(units = dense_units, activation = 'relu'))\n",
    "    model.add(Dense(units = 2, activation = 'softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = Adadelta(\n",
    "       learning_rate=1, name=\"Adadelta\"\n",
    "    ), metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_age(filters = [100], kernel_size = [50], strides = [100], \n",
    "                 dropout_rate = 0.5, pool_size = [5], dense_units = 100, max_len = 1000):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # conv 1\n",
    "    model.add(Conv1D(filters = filters[0], \n",
    "                     kernel_size = kernel_size[0],\n",
    "                     strides = strides[0], \n",
    "                     activation = 'relu', \n",
    "                     input_shape = (max_len, 1) ))\n",
    "\n",
    "    # pooling layer 1\n",
    "    for i in range(len(pool_size)):\n",
    "        model.add(MaxPooling1D(pool_size = pool_size[i], strides = 1))\n",
    "        model.add(Activation('relu'))\n",
    "    \n",
    "    #model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    if dropout_rate is not None:\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "    model.add(Dense(units = dense_units, activation = 'relu'))\n",
    "    model.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = Adadelta(\n",
    "       learning_rate=1, name=\"Adadelta\"\n",
    "    ), metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(\"/home/alice/Pesquisa/tcc-v2/particoes/blogsetbr/houdout/train.csv\") #random state is a seed value\n",
    "df_test=pd.read_csv(\"/home/alice/Pesquisa/tcc-v2/particoes/blogsetbr/houdout/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 70 calls to <function Model.make_test_function.<locals>.test_function at 0x7f9b24c53ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f9b24dd4550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Best parameters set:\n",
      "\tbatch_size: 64\n",
      "\tdropout_rate: 0.1\n",
      "\tepochs: 10\n",
      "\tfilters: [60]\n",
      "\tkernel_size: [1]\n",
      "\tpool_size: [4]\n",
      "\tstrides: [1]\n",
      "0.728893797260978\n"
     ]
    }
   ],
   "source": [
    "#gridsearch para genero\n",
    "X_train = df_train[\"Texts\"].apply(clean_text).to_numpy()\n",
    "X_test = df_test[\"Texts\"].apply(clean_text).to_numpy()\n",
    "y_train = df_train[\"GenderClass\"].to_numpy()\n",
    "y_test = df_test[\"GenderClass\"].to_numpy()\n",
    "\n",
    "num_words = []\n",
    "for text in (X_train.tolist()+X_test.tolist()):\n",
    "    num_words.append(len(text.split()))\n",
    "mean = sum(num_words)//len(num_words)\n",
    "\n",
    "train_texts = X_train.tolist()\n",
    "test_texts = X_test.tolist()\n",
    "\n",
    "tfidfvec = TfidfVectorizer(max_features = mean, max_df=0.9)\n",
    "tfidfvec.fit(train_texts)\n",
    "tfidf_train = tfidfvec.transform(train_texts).toarray()\n",
    "tfidf_test = tfidfvec.transform(test_texts).toarray()\n",
    "\n",
    "X_train = tfidf_train.reshape(tfidf_train.shape[0],tfidf_train.shape[1],1)\n",
    "X_test = tfidf_test.reshape(tfidf_test.shape[0],tfidf_test.shape[1],1)\n",
    "y_train = keras.utils.to_categorical(y_train,num_classes=2)\n",
    "y_test = keras.utils.to_categorical(y_test,num_classes=2)\n",
    "\n",
    "size = X_test.shape[1]\n",
    "\n",
    "CNN = KerasClassifier(model=create_model, \n",
    "                epochs=1,\n",
    "                dropout_rate=0.1,\n",
    "                filters = 60,\n",
    "                kernel_size = 1,\n",
    "                strides = 2,\n",
    "                pool_size = 4,\n",
    "                max_len=X_train.shape[1],\n",
    "                batch_size=32,\n",
    "                verbose=0,\n",
    "                validation_split=0.1\n",
    "                )\n",
    "\n",
    "params_grid = dict(\n",
    "        filters = [[60], [100]],\n",
    "        kernel_size = [[1], [3], [5]],\n",
    "        strides = [[1], [2],[4]],\n",
    "        dropout_rate = [0.1,0.4,0.6],\n",
    "        pool_size = [[4,2],[4],[8,2],[8]],\n",
    "        epochs = [1, 10],\n",
    "        batch_size = [32, 64]\n",
    ")              \n",
    "\n",
    "grid_search = GridSearchCV(CNN, \n",
    "                           params_grid, \n",
    "                           scoring='f1_macro', cv=10, \n",
    "                           return_train_score=True\n",
    "                           )\n",
    "\n",
    "grid_results = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(params_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "\tbatch_size: 32\n",
      "\tdropout_rate: 0.6\n",
      "\tepochs: 10\n",
      "\tfilters: [60]\n",
      "\tkernel_size: [5]\n",
      "\tpool_size: [8, 2]\n",
      "\tstrides: [4]\n",
      "0.4535416477980448\n"
     ]
    }
   ],
   "source": [
    "#gridsearch para idade\n",
    "X_train = df_train[\"Texts\"].apply(clean_text).to_numpy()\n",
    "X_test = df_test[\"Texts\"].apply(clean_text).to_numpy()\n",
    "y_train = df_train[\"AgeClass\"].to_numpy()\n",
    "y_test = df_test[\"AgeClass\"].to_numpy()\n",
    "\n",
    "num_words = []\n",
    "for text in (X_train.tolist()+X_test.tolist()):\n",
    "    num_words.append(len(text.split()))\n",
    "mean = sum(num_words)//len(num_words)\n",
    "\n",
    "train_texts = X_train.tolist()\n",
    "test_texts = X_test.tolist()\n",
    "\n",
    "tfidfvec = TfidfVectorizer(max_features = mean, max_df=0.9)\n",
    "tfidfvec.fit(train_texts)\n",
    "tfidf_train = tfidfvec.transform(train_texts).toarray()\n",
    "tfidf_test = tfidfvec.transform(test_texts).toarray()\n",
    "\n",
    "X_train = tfidf_train.reshape(tfidf_train.shape[0],tfidf_train.shape[1],1)\n",
    "X_test = tfidf_test.reshape(tfidf_test.shape[0],tfidf_test.shape[1],1)\n",
    "y_train = keras.utils.to_categorical(y_train,num_classes=3)\n",
    "y_test = keras.utils.to_categorical(y_test,num_classes=3)\n",
    "\n",
    "size = X_test.shape[1]\n",
    "\n",
    "CNN = KerasClassifier(model=create_model_age, \n",
    "                epochs=1,\n",
    "                dropout_rate=0.1,\n",
    "                filters = 60,\n",
    "                kernel_size = 1,\n",
    "                strides = 2,\n",
    "                pool_size = 4,\n",
    "                max_len=X_train.shape[1],\n",
    "                batch_size=32,\n",
    "                verbose=0,\n",
    "                validation_split=0.1\n",
    "                )\n",
    "\n",
    "params_grid = dict(\n",
    "        filters = [[60], [100]],\n",
    "        kernel_size = [[1], [3], [5]],\n",
    "        strides = [[1], [2],[4]],\n",
    "        dropout_rate = [0.1,0.4,0.6],\n",
    "        pool_size = [[4,2],[4],[8,2],[8]],\n",
    "        epochs = [1, 10],\n",
    "        batch_size = [32, 64]\n",
    ")              \n",
    "\n",
    "grid_search = GridSearchCV(CNN, \n",
    "                           params_grid, \n",
    "                           scoring='f1_macro', cv=10, \n",
    "                           return_train_score=True\n",
    "                           )\n",
    "\n",
    "grid_results = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(params_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "64/64 [==============================] - 47s 724ms/step - loss: 0.6241 - accuracy: 0.6734 - val_loss: 0.8199 - val_accuracy: 0.5049\n",
      "Epoch 2/2\n",
      "64/64 [==============================] - 46s 721ms/step - loss: 0.5284 - accuracy: 0.7335 - val_loss: 0.5222 - val_accuracy: 0.7495\n",
      "16/16 [==============================] - 1s 87ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7335697409309273"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.read_csv(\"/home/alice/Pesquisa/tcc-v2/particoes/blogsetbr/houdout/train.csv\") #random state is a seed value\n",
    "df_test=pd.read_csv(\"/home/alice/Pesquisa/tcc-v2/particoes/blogsetbr/houdout/test.csv\")\n",
    "\n",
    "X_train = df_train[\"Texts\"].apply(clean_text).to_numpy()\n",
    "X_test = df_test[\"Texts\"].apply(clean_text).to_numpy()\n",
    "y_train_gender = df_train[\"GenderClass\"].to_numpy()\n",
    "y_test_gender = df_test[\"GenderClass\"].to_numpy()\n",
    "\n",
    "num_words = []\n",
    "for text in (X_train.tolist()+X_test.tolist()):\n",
    "    num_words.append(len(text.split()))\n",
    "\n",
    "mean = sum(num_words)//len(num_words)\n",
    "\n",
    "train_texts = X_train.tolist()\n",
    "test_texts = X_test.tolist()\n",
    "\n",
    "tfidfvec = TfidfVectorizer(max_features = mean, max_df=0.9)\n",
    "tfidfvec.fit(train_texts)\n",
    "tfidf_train = tfidfvec.transform(train_texts).toarray()\n",
    "tfidf_test = tfidfvec.transform(test_texts).toarray()\n",
    "\n",
    "X_train = tfidf_train.reshape(tfidf_train.shape[0],tfidf_train.shape[1],1)\n",
    "X_test = tfidf_test.reshape(tfidf_test.shape[0],tfidf_test.shape[1],1)\n",
    "\n",
    "y_train_gander_cat = keras.utils.to_categorical(y_train_gender,num_classes=2)\n",
    "y_test_gender_cat = keras.utils.to_categorical(y_test_gender,num_classes=2)\n",
    "\n",
    "size = X_test.shape[1]\n",
    "\n",
    "model_gender = create_model(filters = [100], kernel_size = [3], strides = [1], dropout_rate = 0.4, pool_size = [4], dense_units = 512, max_len = size)\n",
    "\n",
    "model_gender.fit(X_train,y_train_gander_cat,validation_data=(X_test,y_test_gender_cat), batch_size=32, epochs=2)\n",
    "\n",
    "y_pred = model_gender.predict(\n",
    "    X_test\n",
    ")\n",
    "\n",
    "y_pred_list = [np.argmax(x, axis=-1) for x in y_pred]\n",
    "\n",
    "df_test[\"PredictGender\"] = y_pred_list\n",
    "\n",
    "metrics.f1_score(y_test_gender, y_pred_list, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "\tbatch_size: 32\n",
      "\tdropout_rate: 0.6\n",
      "\tepochs: 10\n",
      "\tfilters: [60]\n",
      "\tkernel_size: [1]\n",
      "\tpool_size: [4, 2]\n",
      "\tstrides: [2]\n",
      "0.47766647032443615\n"
     ]
    }
   ],
   "source": [
    "#gridsearch para feminino\n",
    "df_train_fem = df_train[df_train[\"GenderClass\"]==1]\n",
    "df_test_fem = df_test[df_test[\"PredictGender\"]==1]\n",
    "\n",
    "X_train_fem = df_train_fem[\"Texts\"].apply(clean_text).to_numpy()\n",
    "y_train_fem = df_train_fem[\"AgeClass\"].to_numpy()\n",
    "\n",
    "X_test_fem = df_test_fem[\"Texts\"].apply(clean_text).to_numpy()\n",
    "y_test_fem = df_test_fem[\"AgeClass\"].to_numpy()\n",
    "\n",
    "num_words_fem = []\n",
    "for text in (X_train_fem.tolist()+X_test_fem.tolist()):\n",
    "    num_words_fem.append(len(text.split()))\n",
    "mean_fem = sum(num_words_fem)//len(num_words_fem)\n",
    "\n",
    "train_texts_fem = X_train_fem.tolist()\n",
    "test_texts_fem = X_test_fem.tolist()\n",
    "\n",
    "tfidfvec_fem = TfidfVectorizer(max_features = mean_fem, max_df=0.9)\n",
    "tfidfvec_fem.fit(train_texts_fem)\n",
    "tfidf_train_fem = tfidfvec_fem.transform(train_texts_fem).toarray()\n",
    "tfidf_test_fem = tfidfvec_fem.transform(test_texts_fem).toarray()\n",
    "\n",
    "X_train_fem = tfidf_train_fem.reshape(tfidf_train_fem.shape[0],tfidf_train_fem.shape[1],1)\n",
    "X_test_fem = tfidf_test_fem.reshape(tfidf_test_fem.shape[0],tfidf_test_fem.shape[1],1)\n",
    "y_train_fem = keras.utils.to_categorical(y_train_fem,num_classes=3)\n",
    "y_test_fem_cat = keras.utils.to_categorical(y_test_fem,num_classes=3)\n",
    "size_fem = X_test_fem.shape[1]\n",
    "\n",
    "CNN = KerasClassifier(model=create_model_age, \n",
    "                epochs=1,\n",
    "                dropout_rate=0.1,\n",
    "                filters = 60,\n",
    "                kernel_size = 1,\n",
    "                strides = 2,\n",
    "                pool_size = 4,\n",
    "                max_len=X_train_fem.shape[1],\n",
    "                batch_size=32,\n",
    "                verbose=0,\n",
    "                validation_split=0.1\n",
    "                )\n",
    "\n",
    "params_grid = dict(\n",
    "        filters = [[60], [100]],\n",
    "        kernel_size = [[1], [3], [5]],\n",
    "        strides = [[1], [2],[4]],\n",
    "        dropout_rate = [0.1,0.4,0.6],\n",
    "        pool_size = [[4,2],[4],[8,2],[8]],\n",
    "        epochs = [1, 10],\n",
    "        batch_size = [32, 64]\n",
    ")              \n",
    "\n",
    "grid_search = GridSearchCV(CNN, \n",
    "                           params_grid, \n",
    "                           scoring='f1_macro', cv=10, \n",
    "                           return_train_score=True\n",
    "                           )\n",
    "\n",
    "grid_results = grid_search.fit(X_train_fem, y_train_fem)\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(params_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "\tbatch_size: 32\n",
      "\tdropout_rate: 0.6\n",
      "\tepochs: 10\n",
      "\tfilters: [60]\n",
      "\tkernel_size: [1]\n",
      "\tpool_size: [4]\n",
      "\tstrides: [1]\n",
      "0.44442063296605705\n"
     ]
    }
   ],
   "source": [
    "#gridsearch para masculino\n",
    "df_train_masc = df_train[df_train[\"GenderClass\"]==0]\n",
    "df_test_masc = df_test[df_test[\"PredictGender\"]==0]\n",
    "\n",
    "X_train_masc = df_train_masc[\"Texts\"].apply(clean_text).to_numpy()\n",
    "y_train_masc = df_train_masc[\"AgeClass\"].to_numpy()\n",
    "\n",
    "X_test_masc = df_test_masc[\"Texts\"].apply(clean_text).to_numpy()\n",
    "y_test_masc = df_test_masc[\"AgeClass\"].to_numpy()\n",
    "\n",
    "num_words_masc = []\n",
    "for text in (X_train_masc.tolist()+X_test_masc.tolist()):\n",
    "    num_words_masc.append(len(text.split()))\n",
    "mean_masc = sum(num_words_masc)//len(num_words_masc)\n",
    "\n",
    "train_texts_masc = X_train_masc.tolist()\n",
    "test_texts_masc = X_test_masc.tolist()\n",
    "\n",
    "tfidfvec_masc = TfidfVectorizer(max_features = mean_masc, max_df=0.9)\n",
    "tfidfvec_masc.fit(train_texts_masc)\n",
    "tfidf_train_masc = tfidfvec_masc.transform(train_texts_masc).toarray()\n",
    "tfidf_test_masc = tfidfvec_masc.transform(test_texts_masc).toarray()\n",
    "\n",
    "X_train_masc = tfidf_train_masc.reshape(tfidf_train_masc.shape[0],tfidf_train_masc.shape[1],1)\n",
    "X_test_masc = tfidf_test_masc.reshape(tfidf_test_masc.shape[0],tfidf_test_masc.shape[1],1)\n",
    "y_train_masc = keras.utils.to_categorical(y_train_masc,num_classes=3)\n",
    "y_test_masc_cat = keras.utils.to_categorical(y_test_masc,num_classes=3)\n",
    "size_masc = X_test_masc.shape[1]\n",
    "\n",
    "CNN = KerasClassifier(model=create_model_age, \n",
    "                epochs=1,\n",
    "                dropout_rate=0.1,\n",
    "                filters = 60,\n",
    "                kernel_size = 1,\n",
    "                strides = 2,\n",
    "                pool_size = 4,\n",
    "                max_len=X_train_masc.shape[1],\n",
    "                batch_size=32,\n",
    "                verbose=0,\n",
    "                validation_split=0.1\n",
    "                )\n",
    "\n",
    "params_grid = dict(\n",
    "        filters = [[60], [100]],\n",
    "        kernel_size = [[1], [3], [5]],\n",
    "        strides = [[1], [2],[4]],\n",
    "        dropout_rate = [0.1,0.4,0.6],\n",
    "        pool_size = [[4,2],[4],[8,2],[8]],\n",
    "        epochs = [1, 10],\n",
    "        batch_size = [32, 64]\n",
    ")              \n",
    "\n",
    "grid_search = GridSearchCV(CNN, \n",
    "                           params_grid, \n",
    "                           scoring='f1_macro', cv=10, \n",
    "                           return_train_score=True\n",
    "                           )\n",
    "\n",
    "grid_results = grid_search.fit(X_train_masc, y_train_masc)\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(params_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "print(grid_search.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac26e9ee9434047e2b6179ca7c094dd6d7a5c4307efa62cbdc343ee78cec5e23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
